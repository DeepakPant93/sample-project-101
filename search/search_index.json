{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"sample-project-101","text":"<p>A simple project with the DVC integration This repository contains a sample Data Science application built with FastAPI, designed to streamline model training and prediction processes via RESTful APIs. The application leverages Poetry for dependency management, ensuring a robust and scalable development environment.</p>"},{"location":"#features","title":"Features","text":""},{"location":"#fastapi-endpoints","title":"FastAPI Endpoints:","text":"<ul> <li><code>/train-model</code>: API endpoint to initiate model training with provided data and configurations.</li> <li><code>/predict</code>: API endpoint for generating predictions using the trained model.</li> </ul>"},{"location":"#poetry-for-dependency-management","title":"Poetry for Dependency Management:","text":"<ul> <li>Simplifies package installation and management.</li> <li>Ensures compatibility and reproducibility of the project environment.</li> </ul>"},{"location":"#scalable-architecture","title":"Scalable Architecture:","text":"<ul> <li>Modular design with clear separation of concerns.</li> <li>Easy integration of new features or pipelines.</li> </ul>"},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Python &gt;= 3.12</li> <li>Poetry installed (<code>pip install poetry</code>)</li> </ul>"},{"location":"#installation","title":"Installation","text":"<ol> <li> <p>Clone the repository:</p> <pre><code>git clone https://github.com/DeepakPant93/sample-project-101.git\ncd sample-project-101\n</code></pre> </li> <li> <p>Install dependencies using Poetry:</p> <pre><code>poetry install\n</code></pre> </li> <li> <p>Activate the virtual environment:</p> <pre><code>poetry shell\n</code></pre> </li> <li> <p>Run the FastAPI server:</p> <pre><code>uvicorn src.main:app --reload\n</code></pre> </li> </ol>"},{"location":"#api-endpoints","title":"API Endpoints","text":""},{"location":"#train-model","title":"<code>/train-model</code>","text":"<ul> <li>Method: POST</li> <li>Description: Triggers the model training process using provided training data and configuration.</li> <li>Input: JSON object containing training data and optional hyperparameters.</li> <li>Output: Success or error message indicating the status of training.</li> </ul>"},{"location":"#predict","title":"<code>/predict</code>","text":"<ul> <li>Method: POST</li> <li>Description: Generates predictions for new data using the trained model.</li> <li>Input: JSON object containing features for prediction.</li> <li>Output: Prediction results, including probabilities or class labels.</li> </ul>"},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>src/\n\u251c\u2500\u2500 config/          # Configuration files and settings\n\u251c\u2500\u2500 components/      # Reusable components for the application\n\u251c\u2500\u2500 constants/       # Static constants and enumerations\n\u251c\u2500\u2500 entity/          # Definitions of data models and schemas\n\u251c\u2500\u2500 exception/       # Custom exception classes for error handling\n\u251c\u2500\u2500 logger/         # Logging setup for the application\n\u251c\u2500\u2500 pipeline/        # Data science pipeline modules (ingestion, validation, training, etc.)\n\u251c\u2500\u2500 routes/          # API route definitions\n\u251c\u2500\u2500 utils/           # Utility functions (e.g., file handling, data encoding)\n\u2514\u2500\u2500 main.py          # Entry point for the FastAPI application\n</code></pre> <p>Enjoy building with this Data Science FastAPI application! \ud83d\ude80</p>"},{"location":"modules/","title":"Modules","text":""},{"location":"modules/#sample_project_101.__version__","title":"<code>__version__ = '0.0.16'</code>  <code>module-attribute</code>","text":"<p>This project is a Data Science application built with FastAPI, designed to facilitate model training, prediction, and data processing. The application uses Poetry for dependency management and follows modular design principles to ensure scalability and maintainability.</p> <p>Modules:</p> Name Description <code>configuration</code> <p>Contains configuration settings for the application, including environment variables, paths, and system-specific configurations.</p> <code>component</code> <p>Includes reusable components and helper classes for various processes, such as data manipulation and interaction with external services.</p> <code>constant</code> <p>Defines constant values, enumerations, and other static data used throughout the application.</p> <code>utils</code> <p>Contains utility functions that perform common tasks such as reading and writing files, processing images, and other system operations.</p> <code>pipeline</code> <p>Implements the core stages of the data science pipeline, including data ingestion, validation, transformation, model training, and evaluation.</p> <code>routes</code> <p>Defines the FastAPI routes for triggering model training, making predictions, and providing health checks for the application.</p> Features <ul> <li>Model training and prediction workflows via FastAPI.</li> <li>Modular and extensible design to add new features.</li> <li>Dependency management with Poetry.</li> <li>Built for data scientists and developers working with machine learning models in production environments.</li> </ul> <p>This application is designed to help automate and streamline the workflow of training machine learning models and making predictions via an API interface, with easy-to-understand routes and clear separation of concerns between different modules.</p>"},{"location":"modules/#configuration","title":"Configuration","text":"<p>This module centralizes all configuration settings for the project, ensuring consistent and maintainable configuration management. It provides access to environment variables, default settings, and other configurations required across different parts of the application.</p> <p>Modules:</p> Name Description <code>env_config</code> <p>Handles environment-specific configurations, such as development, testing, and production settings.</p> <code>app_config</code> <p>Contains application-wide constants, such as API keys, logging settings, and feature flags.</p> <code>db_config</code> <p>Stores database connection settings, including connection strings and timeout durations.</p> Usage <p>Import the required configuration settings as needed:</p> <p>Example:     <pre><code>from config.env_config import ENVIRONMENT, DEBUG_MODE\nfrom config.db_config import DATABASE_URL\n</code></pre></p> Features <ul> <li>Simplifies access to configuration values across the project.</li> <li>Ensures separation of environment-specific settings from the codebase.</li> <li>Supports overriding default settings via environment variables.</li> </ul> Purpose <ul> <li>Provides a centralized location for all project settings to improve maintainability.</li> <li>Makes it easier to adapt the application to different environments or deployment scenarios.</li> </ul>"},{"location":"modules/#constants","title":"Constants","text":"<p>This package defines global constants used throughout the project. Constants help in maintaining consistency and avoiding magic numbers or strings in the codebase.</p> Usage <p>Import the required constants as needed:</p> <p>Example:     <pre><code>from constants import APP_NAME, ENVIRONMENT\nfrom constants import STATUS_OK, STATUS_BAD_REQUEST\n</code></pre></p> Purpose <ul> <li>Centralizes constant values for maintainability and reusability.</li> <li>Reduces hard-coded values in the project.</li> </ul>"},{"location":"modules/#components","title":"Components","text":"<p>This package contains modular and reusable components used across the project. Each component is designed to encapsulate specific functionality, promoting code reuse, scalability, and maintainability.</p> <p>Modules:</p> Name Description <code>api_component</code> <p>Encapsulates functionality for interacting with external APIs.</p> Usage <p>Import and use the required components as needed:</p> <p>Example:     <pre><code>from components.api_component import APIManager\n</code></pre></p> Purpose <ul> <li>Organizes project functionality into self-contained, reusable components.</li> <li>Promotes modular design principles, making the project easier to extend and maintain.</li> <li>Ensures separation of concerns by isolating specific functionalities into dedicated modules.</li> </ul>"},{"location":"modules/#logger","title":"Logger","text":"<p>This module provides centralized logging utilities for the data science pipeline. It standardizes logging practices, ensures consistency across components, and facilitates easy debugging and monitoring of the pipeline's execution, including data preprocessing, model training, evaluation, and predictions.</p> <p>Functions:</p> Name Description <code>setup_logging</code> <p>Configures the logging system, including log format, level, and output destinations.</p> <code>get_logger</code> <p>Returns a logger instance for a specific module or stage of the pipeline.</p> Features <ul> <li>Centralized logging configuration to maintain consistency.</li> <li>Support for different log levels (INFO, DEBUG, WARNING, ERROR, CRITICAL).</li> <li>Ability to write logs to files, console, or external monitoring systems.</li> <li>Timestamped log entries for accurate tracking of events.</li> <li>Integration with custom exception handling for detailed error reporting.</li> </ul> Usage <p>Use this module to log messages in a standardized manner across the project:</p> <p>Example:     <pre><code>from src.logging import logger\n\nlogger.info(\"Starting the model training process...\")\nlogger.error(\"An error occurred during data validation.\")\n</code></pre></p> Purpose <ul> <li>To provide a standardized mechanism for logging messages throughout the data science pipeline.</li> <li>To assist in debugging by capturing detailed logs of each pipeline stage.</li> <li>To enable seamless integration with monitoring and alerting systems.</li> </ul> Best Practices <ul> <li>Use appropriate log levels to categorize messages (e.g., DEBUG for detailed information, ERROR for issues).</li> <li>Ensure logs include sufficient context, such as function names or input details, to aid debugging.</li> <li>Regularly monitor log files for anomalies or errors in the pipeline.</li> </ul> Additional Notes <ul> <li>The <code>setup_logging</code> function can be configured to write logs to multiple destinations, such as files or cloud services.</li> <li>The module can be extended to integrate with third-party monitoring tools like Elasticsearch, Splunk, or Datadog.</li> </ul>"},{"location":"modules/#utils","title":"Utils","text":"<p>The <code>utils</code> module provides various utility functions for file I/O, data encoding/decoding, and directory management.</p> <p>Functions:</p> Name Description <code>read_yaml</code> <p>Reads a YAML file and returns its contents as a dictionary.</p> <code>create_directories</code> <p>Creates directories if they do not exist.</p> <code>save_json</code> <p>Saves data to a JSON file.</p> <code>load_json</code> <p>Loads JSON data from a file.</p> <code>save_bin</code> <p>Saves binary data to a file.</p> <code>load_bin</code> <p>Loads binary data from a file.</p> <code>get_size</code> <p>Returns the size of a file or directory in bytes.</p> <code>decode_image</code> <p>Decodes an image from a base64 string.</p> <code>encode_image_into_base64</code> <p>Encodes an image into a base64 string.</p>"},{"location":"modules/#exceptions","title":"Exceptions","text":"<p>This module defines custom exception classes and error-handling utilities tailored to the needs of a data science pipeline. It helps standardize error reporting, improve debugging, and provide meaningful feedback during model training, data preprocessing, and prediction processes.</p> <p>Classes:</p> Name Description <code>DataValidationError</code> <p>Raised when input data fails validation checks.</p> <code>ModelTrainingError</code> <p>Raised during errors in the model training phase, such as convergence issues or invalid configurations.</p> <code>PredictionError</code> <p>Raised when the prediction pipeline encounters issues, such as missing features or incompatible input formats.</p> <code>PipelineExecutionError</code> <p>Raised for generic errors occurring during pipeline execution.</p> Usage <p>Import and use the exceptions in various stages of the data science pipeline:</p> <p>Example:     <pre><code>from exception import DataValidationError, ModelTrainingError\n\ntry:\n    validate_data(input_data)\nexcept DataValidationError as e:\n    logger.error(f\"Data validation failed: {e}\")\n    raise\n</code></pre></p> Features <ul> <li>Custom exceptions for specific pipeline stages, ensuring meaningful error reporting.</li> <li>Enables targeted exception handling, reducing debugging time.</li> <li>Provides a consistent structure for error messages across the project.</li> </ul> Purpose <ul> <li>To define project-specific exceptions for common error scenarios in the pipeline.</li> <li>To improve the robustness and reliability of the pipeline by enabling clear error handling.</li> <li>To make the debugging process more intuitive by raising descriptive errors.</li> </ul> <p>Examples:</p> <ul> <li>Data Validation: Raise a <code>DataValidationError</code> if the input data schema is incorrect or missing required fields.</li> <li>Model Training: Raise a <code>ModelTrainingError</code> if the model fails to converge due to invalid hyperparameters.</li> <li>Prediction: Raise a <code>PredictionError</code> when incompatible input data is passed to the model.</li> </ul> Additional Notes <ul> <li>Use these exceptions in conjunction with logging to provide detailed error information.</li> <li>Ensure that custom exceptions are raised with meaningful messages to assist in debugging and error resolution.</li> </ul>"},{"location":"modules/#sample_project_101.exception.CustomException","title":"<code>CustomException</code>","text":"<p>               Bases: <code>HTTPException</code></p> Source code in <code>sample_project_101/exception/__init__.py</code> <pre><code>class CustomException(HTTPException):\n    def __init__(self, status_code: int, detail: str):\n        \"\"\"\n        Custom exception for handling API errors.\n\n        :param status_code: The HTTP status code to return.\n        :param detail: A string describing the error in detail.\n        \"\"\"\n        super().__init__(status_code=status_code, detail=detail)\n</code></pre>"},{"location":"modules/#sample_project_101.exception.CustomException.__init__","title":"<code>__init__(status_code, detail)</code>","text":"<p>Custom exception for handling API errors.</p> <p>:param status_code: The HTTP status code to return. :param detail: A string describing the error in detail.</p> Source code in <code>sample_project_101/exception/__init__.py</code> <pre><code>def __init__(self, status_code: int, detail: str):\n    \"\"\"\n    Custom exception for handling API errors.\n\n    :param status_code: The HTTP status code to return.\n    :param detail: A string describing the error in detail.\n    \"\"\"\n    super().__init__(status_code=status_code, detail=detail)\n</code></pre>"},{"location":"modules/#entities","title":"Entities","text":"<p>This module defines the core entities and data structures used throughout the data science pipeline. Entities are designed to represent the inputs, outputs, and intermediate states of the model training and prediction processes, ensuring consistency and validation across the project.</p> <p>Modules:</p> Name Description <code>data_schema</code> <p>Contains definitions for input data schemas, ensuring validation and compatibility with the pipeline.</p> <code>model_params</code> <p>Defines structures for storing model parameters, hyperparameters, and configuration settings.</p> <code>prediction_result</code> <p>Provides entities for representing and managing prediction outputs, including probabilities and metadata.</p> Usage <p>Import and use the required entities in your data science pipeline:</p> <p>Example:     <pre><code>from entity.data_schema import InputSchema\nfrom entity.model_params import ModelConfig\nfrom entity.prediction_result import PredictionOutput\n</code></pre></p> Features <ul> <li>Defines standardized data structures for inputs, outputs, and parameters.</li> <li>Ensures validation and consistency in data passed through the pipeline.</li> <li>Promotes maintainability and readability by using clear, reusable entities.</li> </ul> Purpose <ul> <li>Serves as a single source of truth for defining data structures in the pipeline.</li> <li>Facilitates seamless integration between different stages of the pipeline,   such as data ingestion, validation, model training, and prediction.</li> <li>Improves error handling by validating data early in the process.</li> </ul> <p>Examples:</p> <ul> <li>Data Schema: Define the expected input structure for data preprocessing.</li> <li>Model Parameters: Store configurations like learning rate, batch size,   and optimizer type.</li> <li>Prediction Results: Represent the model's outputs in a structured format,   including predicted classes, probabilities, and confidence scores.</li> </ul>"},{"location":"modules/#pipeline","title":"Pipeline","text":"<p>The <code>pipeline</code> module orchestrates the end-to-end flow of the data science process, from raw data ingestion to final predictions. It consists of multiple submodules, each responsible for a specific stage in the pipeline. This modular structure ensures scalability, reusability, and ease of maintenance. The pipeline is designed to handle data preprocessing, model training, evaluation, and predictions in a systematic and automated manner.</p> <p>Modules:</p> Name Description <code>Data-Ingestion</code> <p>Collects and ingests raw data from various sources, performs basic checks, and stores it in a structured format.</p> <code>Data-Validation</code> <p>Validates ingested data for correctness, completeness, and consistency, ensuring it meets predefined quality standards.</p> <code>Data-Transformation</code> <p>Transforms validated data into a format suitable for model training, including feature engineering, scaling, encoding, and preprocessing.</p> <code>Model-Training</code> <p>Trains machine learning models using transformed data, supports hyperparameter tuning, saving trained models, and logging metrics.</p> <code>Model-Evaluation</code> <p>Evaluates trained models on a validation or test dataset, providing detailed performance metrics and insights.</p> <code>Prediction</code> <p>Uses trained models to make predictions on new or unseen data, including batch or real-time inference and post-processing of predictions.</p> Features <ul> <li>Modular architecture for each pipeline stage, ensuring maintainability and reusability.</li> <li>Support for extensive logging and error handling at each stage.</li> <li>Flexibility to customize and extend pipeline stages as needed.</li> <li>Compatibility with various data formats and storage systems.</li> </ul> Usage <p>Import and use specific pipeline stages or run the entire pipeline end-to-end:</p> <p>Example:     <pre><code>from pipeline.stage_01_data_ingestion import DataIngestionTrainingPipeline\nfrom pipeline.stage_04_model_trainer import ModelTrainingPipeline\n\n# Perform data ingestion\ndata_ingestion = DataIngestionTrainingPipeline(config)\nraw_data = data_ingestion.run()\n\n# Train the model\nmodel_trainer = ModelTrainingPipeline(config, raw_data)\ntrained_model = model_trainer.run()\n</code></pre></p> Purpose <ul> <li>To streamline the execution of a data science workflow, reducing manual intervention.</li> <li>To ensure consistency and traceability of processes across multiple runs.</li> <li>To provide reusable components for different machine learning projects.</li> </ul>"},{"location":"modules/#routes","title":"Routes","text":"<p>The <code>routes</code> module defines the API routes that enable interaction with the machine learning model. This module contains endpoints for initiating model training and making predictions on new data.</p> Endpoints <ul> <li>POST /train-model: This endpoint is responsible for triggering the model training process.</li> <li>POST /predict: This endpoint is responsible for generating predictions using the trained model.</li> </ul> Features <ul> <li>API endpoints for model training and prediction.</li> <li>Flexible and easy-to-extend with additional routes.</li> <li>Integration with the model training pipeline and prediction modules.</li> <li>Handles input validation and error responses for robustness.</li> </ul>"}]}